{
  "fork_families": [],
  "last_updated": "2025-12-30T16:57:33.380502Z",
  "orphaned_forks": [
    {
      "created_at": "2024-06-24T15:01:31Z",
      "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
      "forks_count": 2,
      "full_name": "0gfoundation/vllm",
      "html_url": "https://github.com/0gfoundation/vllm",
      "is_fork": true,
      "language": "Python",
      "last_checked": "2025-12-30T05:29:12.310510Z",
      "name": "vllm",
      "owner": "0gfoundation",
      "parent": "vllm-project/vllm",
      "source": "vllm-project/vllm",
      "stars": 1,
      "updated_at": "2025-08-31T12:04:52Z"
    },
    {
      "created_at": "2024-06-24T15:01:31Z",
      "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
      "forks_count": 2,
      "full_name": "0glabs/vllm",
      "html_url": "https://github.com/0gfoundation/vllm",
      "is_fork": true,
      "language": "Python",
      "last_checked": "2025-12-30T05:31:37.701284Z",
      "name": "vllm",
      "owner": "0gfoundation",
      "parent": "vllm-project/vllm",
      "source": "vllm-project/vllm",
      "stars": 1,
      "updated_at": "2025-08-31T12:04:52Z"
    }
  ],
  "repo_name": "vllm",
  "total_repos": 2
}